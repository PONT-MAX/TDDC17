package tddc17;import aima.core.environment.liuvacuum.*;import aima.core.agent.Action;import aima.core.agent.AgentProgram;import aima.core.agent.Percept;import aima.core.agent.impl.*;import java.util.Random;import java.util.Vector;import java.util.Arrays;class MyAgentState{	public int[][] world = new int[30][30];	public int initialized = 0;	final int UNKNOWN 	= 0;	final int WALL 		= 1;	final int CLEAR 	= 2;	final int DIRT		= 3;	final int HOME		= 4;	final int ACTION_NONE 			= 0;	final int ACTION_MOVE_FORWARD 	= 1;	final int ACTION_TURN_RIGHT 	= 2;	final int ACTION_TURN_LEFT 		= 3;	final int ACTION_SUCK	 		= 4;	public int AI_STATE	 		= 0;  	final int FIND_WALL 		= 0;	final int FIND_UNKNOWN 		= 1;	final int FIND_HOME	 		= 2;	public int WALL_height = 0;	public int WALL_width = 0;	public int agent_x_position = 1;	public int agent_y_position = 1;	public int agent_x_start = 0;	public int agent_y_start = 0;	public int agent_last_action = ACTION_NONE;	public int agent_last_turn = ACTION_NONE;	public static final int NORTH = 0;	public static final int EAST = 1;	public static final int SOUTH = 2;	public static final int WEST = 3;	public int agent_direction = EAST;	//public Vector<Integer> nextMove_MY = new Vector<Integer>();	MyAgentState()	{		for (int i=0; i < world.length; i++)			for (int j=0; j < world[i].length ; j++)				world[i][j] = UNKNOWN;		world[1][1] = HOME;		agent_last_action = ACTION_NONE;	}	// Based on the last action and the received percept updates the x & y agent position	public void updatePosition(DynamicPercept p)	{		Boolean bump = (Boolean)p.getAttribute("bump");		if (agent_last_action==ACTION_MOVE_FORWARD && !bump)		{			switch (agent_direction) {			case MyAgentState.NORTH:				agent_y_position--;				break;			case MyAgentState.EAST:				agent_x_position++;				break;			case MyAgentState.SOUTH:				agent_y_position++;				break;			case MyAgentState.WEST:				agent_x_position--;				break;			}		}	}	public void updateWorld(int x_position, int y_position, int info)	{		world[x_position][y_position] = info;	}	public void printWorldDebug()	{		for (int i=0; i < world.length; i++)		{			for (int j=0; j < world[i].length ; j++)			{				if (world[j][i]==UNKNOWN)					System.out.print(" ? ");				if (world[j][i]==WALL)					System.out.print("# ");				if (world[j][i]==CLEAR)					System.out.print(" . ");				if (world[j][i]==DIRT)					System.out.print(" D ");				if (world[j][i]==HOME)					System.out.print(" H ");			}			System.out.println("");		}	}}////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////class MyAgentProgram implements AgentProgram {	private int initnialRandomActions = 10;	private Random random_generator = new Random();	public Boolean first_bump = false;	// Here you can define your variables!	public Vector<Integer> nextMove = new Vector<Integer>();	public int iterationCounter = 30*30*20; //change this	public MyAgentState state = new MyAgentState();	public int WALL_width = 0;	public int WALL_height = 0;	// moves the Agent to a random start position	// uses percepts to update the Agent position - only the position, other percepts are ignored	// returns a random action	private Action moveToRandomStartPosition(DynamicPercept percept) {		int action = random_generator.nextInt(6);		initnialRandomActions--;		state.updatePosition(percept);		if(action==0) {			state.agent_direction = ((state.agent_direction-1) % 4);			if (state.agent_direction<0) 				state.agent_direction +=4;			state.agent_last_action = state.ACTION_TURN_LEFT;			return LIUVacuumEnvironment.ACTION_TURN_LEFT;		} else if (action==1) {			state.agent_direction = ((state.agent_direction+1) % 4);			state.agent_last_action = state.ACTION_TURN_RIGHT;			return LIUVacuumEnvironment.ACTION_TURN_RIGHT;		} 		state.agent_last_action=state.ACTION_MOVE_FORWARD;		return LIUVacuumEnvironment.ACTION_MOVE_FORWARD;	}	public Boolean sameTile(int[] a, int[] b){		if (a[0] == b[0] && a[1] == b[1])			return true;		else			return false;	}	//Return Manhattan distance between two tiles.	public int cost_h(int[] tile, int[] target){   		int dx = Math.abs(tile[0]-target[0]);		int dy = Math.abs(tile[1]-target[1]);		return  (int)Math.round(Math.sqrt((double)(dx*dx) + (double)(dy*dy)));	}		public int cost_hm(int[] tile, int[] target){   		int dx = Math.abs(tile[0]-target[0]);		int dy = Math.abs(tile[1]-target[1]);		return  dx+dy;	}	//Calculates cost for a path	public int calculate_path_cost(Vector<int[]> path, int[] target){		int g = path.size() - 1;		//First node is always where we are.		int[] current_node_position = path.get(0); 		int h = cost_h(current_node_position,target);		return g + h;	}	public int get_next_direction(int[] current, int[] next){		//From current to next tile without thinking of rotation (N,E,S,W)		System.out.println("Current = " + Arrays.toString(current));		System.out.println("Next = " + Arrays.toString(next));				int dx = current[0] - next[0];		int dy = current[1] - next[1]; 		System.out.println("GND dx= " + dx + " dy = " +  dy);				if (dy == 1)			return 0;		else if (dx == -1)			return 1;		else if (dy == -1)			return 2;		else			return 3;	}	public void get_next_action(int current, int next){		final int ACTION_MOVE_FORWARD 	= 1;		final int ACTION_TURN_RIGHT 	= 2;		final int ACTION_TURN_LEFT 		= 3;		int delta = current - next;		if (delta == 0){			nextMove.add(ACTION_MOVE_FORWARD);			System.out.println("NA = FORWARD");		}		else if ( (delta == 1) || (delta == -3) ){			System.out.println("NA = LEFT FORWARD");			nextMove.add(ACTION_TURN_LEFT);			nextMove.add(ACTION_MOVE_FORWARD);		}		else if ( (delta == -1) || (delta == 3) ){			System.out.println("NA = RIGHT FORWARD");			nextMove.add(ACTION_TURN_RIGHT);				nextMove.add(ACTION_MOVE_FORWARD);		}		else {			System.out.println("NA = turn back FORWARD");			nextMove.add(ACTION_TURN_RIGHT);			nextMove.add(ACTION_TURN_RIGHT);				nextMove.add(ACTION_MOVE_FORWARD);		}	}	public void A_star_action(Vector<int[]> path, MyAgentState state){		System.out.println("ASa1 PS = " + path.size());		//Remove the tile we're in.		int index_to_last = path.size()-1;		path.remove(index_to_last);		int[] last_node = path.remove(index_to_last-1);								int current_direction = state.agent_direction;		int[] current_position = {state.agent_x_position,state.agent_y_position};				System.out.println("ASa: current_dir = " + current_direction);		int next_direction = get_next_direction(current_position,last_node);		System.out.println("ASa: next_dir = " + next_direction);		get_next_action(current_direction, next_direction);						while (!path.isEmpty()) {			//Save next direction as the current after the iteration.			current_position = last_node;			current_direction = next_direction;			index_to_last = path.size() - 1;			last_node = path.remove(index_to_last);			next_direction = get_next_direction(current_position,last_node);			get_next_action(current_direction,next_direction);		}	}	//Based on two tiles, give a vector with tiles showing the path to target.	public Vector<int[]> A_star(int[] tile, int[] target, MyAgentState state){		System.out.println("A* START: x = " + tile[0] + " y = " + tile[1]);		System.out.println("A* TARGET: x = " + target[0] + " y = " + target[1]);				// The set of nodes already evaluated.		Vector<Vector<int[]>> explored = new Vector<Vector<int[]>>();		// The set of currently discovered nodes still to be evaluated.		// Initially, only the start node is known. Ordered by f, where		// f = g + h, g = distance from start. Nodes in vector minus 1.		// h = heuristic. We use Manhattan distance as heuristic.		Vector<Vector<int[]>> frontier = new Vector<Vector<int[]>>();		Vector<int[]> node = new Vector<int[]>();		node.add(0,tile);		frontier.add(0,node);		Vector<int[]> children = new Vector<int[]>();		int[] child_pos;		while(true){			if(frontier.isEmpty()){				Vector<int[]> failure = new Vector<int[]>();				System.out.println("A* FAILURE!! ");				state.world[target[0]][target[1]] = 1;				return failure;			}			//Pop frontier (Choose node with lowest f)			node = frontier.remove(0); 			//Reached goal			if(sameTile(node.firstElement(),target))				return node;			explored.add(node);			//Add all tiles around node tile as children.			children.clear();			child_pos = node.firstElement(); 			for(int i = 0; i < 4; ++i){				if(i == 0){					int[] pos = { (child_pos[0] + 1), child_pos[1] };					children.add(0,pos);				}				if(i == 1){					int[] pos = { (child_pos[0] - 1), child_pos[1] };					children.add(0,pos);				}				if(i == 2){					int[] pos = { (child_pos[0]), (child_pos[1] +1)};					children.add(0,pos);				}				if(i == 3){					int[] pos = { (child_pos[0] ), (child_pos[1] -1)};					children.add(0,pos);				}			}			Boolean child_exists_in_frontier = false;			Boolean child_exists_in_explored = false;			//System.out.println("A* Node: x = " + node.get(0)[0] + " y = " + node.get(0)[1]);			for(int[] cp : children){				//System.out.println("A* childrenloop: x = " + cp[0] + " y = " + + cp[1]);			}			// Vi har inte tildelat vad childen ska vara frÃ¥n world			for(int i = 0;i<4;++i){				child_exists_in_frontier = false;				child_exists_in_explored = false;				if(state.world[children.get(i)[0]][children.get(i)[1]] == 1 || children.get(i)[0] >= state.WALL_width || children.get(i)[1] >= state.WALL_height || children.get(i)[0] == 0 || children.get(i)[1] == 0){					//System.out.println("A* Child is in WALL " + i + " Child_pos x= " + children.get(i)[0] + " y = "+ children.get(i)[1]);					continue;				}				int child_position;				//Find child in explored tiles.				for (Vector<int[]> path_it : explored) {					for(int[] j : path_it){						if(Arrays.equals(j,children.get(i))){							child_exists_in_explored = true;							break;						}					}					if (child_exists_in_explored ) {						//System.out.println("A* Child " + i + " found in explored");						break;					}				}								//Vector<int[]> path_it_frontier = new Vector<int[]>();				int child_position_frontier = 0;				//Find child in frontier tiles.				for (Vector<int[]> path_it : frontier) {					for(int[] j : path_it){						if(Arrays.equals(j,children.get(i))){							child_exists_in_frontier = true;							break;						}					}					if (child_exists_in_frontier){						//System.out.println("A* Child " + i + " found in frontier");						break;					}				}				if (!(child_exists_in_frontier || child_exists_in_explored)){					//Create node path, add the child first.					Vector<int[]> new_node = new Vector<int[]>(node);					new_node.add(0,children.get(i));					int new_node_cost = calculate_path_cost(new_node,target);										boolean breaked = false;										if(!frontier.isEmpty()){						for (Vector<int[]> path_it : frontier) {							int path_it_cost = calculate_path_cost(path_it, target);							if (path_it_cost >= new_node_cost){								//Insert just before the node with larger cost.								//Why isn't there a good insert function in Java...?								int java_sucks = frontier.indexOf(path_it);								frontier.insertElementAt(new_node,java_sucks);								//System.out.println("A* Frontier insert at = " + java_sucks);								breaked = true;								break;							}						}						if(!breaked){						frontier.insertElementAt(new_node,frontier.size()-1);						//System.out.println("A* Frontier insert at last place = " + (frontier.size()-1));						}					}					else{						frontier.add(new_node);						//System.out.println("A* Frontier empty inserted first= " + (frontier.size()-1));					}				}				else if (child_exists_in_frontier){					//System.out.println("A* Child " + i + " exsist in frontier BADBADBABDBDABDBADBBDBDAB");					//Replace child in frontier if new child is better.					Vector<int[]> new_node = node;					new_node.add(0,children.get(i));					int new_node_cost = calculate_path_cost(new_node,target);					//Calculate old node, must be done differently than the new					// new node as we shouldn't count from the first element.					int h=100,g=100;					Vector<int[]> path_it_frontier;					for (Vector<int[]> path_it : frontier) {						for(int[] j : path_it){							if(Arrays.equals(j,children.get(i))){								g = cost_hm(tile,j);								h = cost_h(j,target);								path_it_frontier = path_it;								break;							}						}					}										int old_node_cost = g + h;					if (new_node_cost < old_node_cost){						path_it_frontier = new_node;					}				}			}		}	}	private Action find_home()	{			//if (!nextMove.isEmpty())		//    return nextMove.remove(0);		//astar dat shit		return NoOpAction.NO_OP;	}	private Action perform_next_action(MyAgentState state){		final int ACTION_MOVE_FORWARD 	= 1;		final int ACTION_TURN_RIGHT 	= 2;		final int ACTION_TURN_LEFT 		= 3;		int next_move = nextMove.remove(0);		if (next_move == ACTION_TURN_RIGHT){			state.agent_direction  = ((((state.agent_direction+1) % 4) + 4) % 4);			state.agent_last_turn=state.ACTION_TURN_RIGHT;			state.agent_last_action=state.ACTION_TURN_RIGHT;			return LIUVacuumEnvironment.ACTION_TURN_RIGHT;		}		else if (next_move == ACTION_TURN_LEFT){			state.agent_direction  = ((((state.agent_direction-1) % 4) + 4) % 4);			state.agent_last_turn=state.ACTION_TURN_LEFT;			state.agent_last_action=state.ACTION_TURN_LEFT;			return LIUVacuumEnvironment.ACTION_TURN_LEFT;		}		else{			state.agent_last_action=state.ACTION_MOVE_FORWARD;			return LIUVacuumEnvironment.ACTION_MOVE_FORWARD;		}	}	private Action find_unknown(MyAgentState state)	{		if (!nextMove.isEmpty()){			return perform_next_action(state);		}		int[] current_point = {state.agent_x_position,state.agent_y_position};		int[] best_point = {100,100}; //Initialize as a large number		int best_point_cost = cost_h(current_point,best_point);		for(int i = 1; i < state.WALL_width; ++i ){			for(int j = 1; j < state.WALL_height; ++j){				if (state.world[i][j] == state.UNKNOWN){					int[] new_point = {i,j};					int new_point_cost = cost_h(current_point,new_point);					if (new_point_cost < best_point_cost){						best_point_cost = new_point_cost;						best_point = new_point;					}				}			}		}		System.out.println("BEST POINT x= " + best_point[0] + " y= " + best_point[1]);		if (best_point[0] == 100){			state.AI_STATE++;			return find_home();		}		else{			System.out.println("Else 1");			Vector<int[]> next_path = A_star(current_point,best_point,state);			if(next_path.isEmpty())				return find_unknown(state);			System.out.println("Else 2 = NP.size = " + next_path.size());			A_star_action(next_path,state);		}		//Todo		if (!nextMove.isEmpty()){			return perform_next_action(state);		}		else{			return LIUVacuumEnvironment.ACTION_TURN_LEFT;		}			}	private Action find_wall(Boolean bump, MyAgentState state)	{		if (bump)		{			state.agent_last_action=state.ACTION_TURN_LEFT;			state.agent_last_turn=state.ACTION_TURN_LEFT;			state.agent_direction = ((((state.agent_direction-1) % 4) + 4) % 4); //Fix direction			//Finds a wall and starts navigating around the map with this as the start position.			if(!first_bump){ 				state.agent_x_start = state.agent_x_position;				state.agent_y_start = state.agent_y_position;				first_bump = true;			} 			return LIUVacuumEnvironment.ACTION_TURN_LEFT;		}		//Finds contours of the map.		if (state.agent_x_start == state.agent_x_position &&				state.agent_y_start == state.agent_y_position )		{			int ww = 100, ew = 0, nw = 100, sw = 0;			for(int i = 0; i < state.world.length; ++i ){				for(int j = 0; j < state.world[0].length; ++j){					if(state.world[i][j] == 1){						if(i < ww)							ww = i;						if(i > ew)							ew = i;						if(j < nw)							nw = j;						if(j > sw)							sw = j;					}				}			}			WALL_width = Math.abs(ww-ew);			WALL_height = Math.abs(nw-sw);			System.out.println("WEST WALL x-led = " + ww);			System.out.println("EAST WALL x-led = " + ew);			System.out.println("N WALL y-led = " + nw);			System.out.println("S WALL y-led = " + sw);			System.out.println("WALL Width = " + WALL_width);			System.out.println("WALL hight = " + WALL_height);			if(WALL_width > 3 && WALL_height > 3){				System.out.println("HITTAT HELA KONTUREN AV BANAN");				state.WALL_width = WALL_width;				state.WALL_height = WALL_height;				System.out.println("WW = " + state.WALL_width);				System.out.println("WH = " + state.WALL_height);				state.AI_STATE++;			}		}		if(state.AI_STATE == state.FIND_WALL){			if(state.agent_last_action == state.ACTION_TURN_LEFT){				//System.out.println("Last LEFT, Now FORWARD");				state.agent_last_action=state.ACTION_MOVE_FORWARD;				return LIUVacuumEnvironment.ACTION_MOVE_FORWARD;			}			else if(state.agent_last_turn==state.ACTION_TURN_LEFT ||					(state.agent_last_action==state.ACTION_MOVE_FORWARD && first_bump)){				//System.out.println("Right undiscoverd, Go right");				state.agent_direction  = ((((state.agent_direction+1) % 4) + 4) % 4);				state.agent_last_turn=state.ACTION_TURN_RIGHT;				state.agent_last_action=state.ACTION_TURN_RIGHT;				return LIUVacuumEnvironment.ACTION_TURN_RIGHT;			}			else{				//System.out.println("Move Forward");				state.agent_last_action=state.ACTION_MOVE_FORWARD;				return LIUVacuumEnvironment.ACTION_MOVE_FORWARD;			}		}		else		{			return find_unknown(state);		}	}	@Override	public Action execute(Percept percept) {		// DO NOT REMOVE this if condition!!!		if (initnialRandomActions>0) {			return moveToRandomStartPosition((DynamicPercept) percept);		} else if (initnialRandomActions==0) {			// process percept for the last step of the initial random actions			initnialRandomActions--;			state.updatePosition((DynamicPercept) percept);			System.out.println("Processing percepts after the last execution of moveToRandomStartPosition()");			state.agent_last_action=state.ACTION_SUCK;			return LIUVacuumEnvironment.ACTION_SUCK;		}		// This example agent program will update the internal agent state while only moving forward.		// START HERE - code below should be modified!		System.out.println("x=" + state.agent_x_position);		System.out.println("y=" + state.agent_y_position);		System.out.println("x_s=" + state.agent_x_start);		System.out.println("y_s=" + state.agent_y_start);		System.out.println("dir=" + state.agent_direction);		iterationCounter--;		if (iterationCounter+1000==0)			return NoOpAction.NO_OP;		DynamicPercept p = (DynamicPercept) percept;		Boolean bump = (Boolean)p.getAttribute("bump");		Boolean dirt = (Boolean)p.getAttribute("dirt");		Boolean home = (Boolean)p.getAttribute("home");		System.out.println("percept: " + p);		// State update based on the percept value and the last action		state.updatePosition((DynamicPercept)percept);		if (bump) {			switch (state.agent_direction) {			case MyAgentState.NORTH:				state.updateWorld(state.agent_x_position,state.agent_y_position-1,state.WALL);				break;			case MyAgentState.EAST:				state.updateWorld(state.agent_x_position+1,state.agent_y_position,state.WALL);				break;			case MyAgentState.SOUTH:				state.updateWorld(state.agent_x_position,state.agent_y_position+1,state.WALL);				break;			case MyAgentState.WEST:				state.updateWorld(state.agent_x_position-1,state.agent_y_position,state.WALL);				break;			}		}		if (dirt)			state.updateWorld(state.agent_x_position,state.agent_y_position,state.DIRT);		else			state.updateWorld(state.agent_x_position,state.agent_y_position,state.CLEAR);		state.printWorldDebug();		// Next action selection based on the percept value		if (dirt)		{			System.out.println("DIRT -> choosing SUCK action!");			state.agent_last_action=state.ACTION_SUCK;			return LIUVacuumEnvironment.ACTION_SUCK;		} 		else if(state.AI_STATE == state.FIND_WALL){	  			return find_wall(bump, state);		}		else if (state.AI_STATE == state.FIND_UNKNOWN){			return find_unknown(state);		}		else if (state.AI_STATE == state.FIND_HOME){			return find_home();		}		System.out.println(state.AI_STATE);		System.out.println("Your algorithm is bad");		return NoOpAction.NO_OP;		}}public class MyVacuumAgent extends AbstractAgent {	public MyVacuumAgent() {		super(new MyAgentProgram());	}}